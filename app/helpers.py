import re
import aiohttp
import bleach
import html
from bs4 import BeautifulSoup
from bs4.element import Tag
from app.database.models.Word import Word
from app.database.redis import redis_store
from app.settings import settings

POST_KEY = "post:{id}"


async def is_duplicate(id: str, original_text: str) -> bool:
    original_text = original_text.lower()
    from loguru import logger

    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Ç–µ–∫—Å—Ç –≤ Redis
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ—Å—Ç—ã
        recent_keys = await redis_store.keys(POST_KEY.format(id="*"))
        
        if recent_keys:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –∫–ª—é—á–µ–π –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            max_check_keys = 100
            if len(recent_keys) > max_check_keys:
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∫–ª—é—á–∏ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–Ω–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã)
                recent_keys = recent_keys[-max_check_keys:]
            
            # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–π
            texts = await redis_store.redis.mget(recent_keys) if recent_keys else []
            
            logger.info(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –¥–ª—è ID: {id}, –ø—Ä–æ–≤–µ—Ä—è–µ–º {len(texts)} –∏–∑ {len(await redis_store.keys(POST_KEY.format(id='*')))} —Ç–µ–∫—Å—Ç–æ–≤ –≤ Redis")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ texts –Ω–µ None –∏ –Ω–µ –ø—É—Å—Ç–æ–π
            if texts:
                for i, text in enumerate(texts):
                    if text and original_text == text:
                        logger.info(f"–ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç! ID: {id}, —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Ç–µ–∫—Å—Ç–æ–º #{i}")
                        return True

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –ø–æ—Å—Ç —Å TTL 2 —á–∞—Å–∞
        await redis_store.set_value_ex(POST_KEY.format(id=id), original_text, 60 * 60 * 2)
        logger.info(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –ø–æ—Å—Ç –≤ Redis: {id} (TTL: 2 —á–∞—Å–∞)")
        return False
    except Exception as e:
        # –ï—Å–ª–∏ Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ª–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")
        return False


async def get_fetched_post_ids():
    keys: list[str] = await redis_store.keys(POST_KEY.format(id="*"))
    return [key.split(":")[1] for key in keys]


async def cleanup_old_redis_data():
    """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Redis –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    from loguru import logger
    
    try:
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø–æ—Å—Ç—ã (—Å—Ç–∞—Ä—à–µ 2 —á–∞—Å–æ–≤)
        deleted_posts = await redis_store.cleanup_old_posts(max_age_hours=2)
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏
        memory_info = await redis_store.get_memory_usage()
        
        logger.info(f"[CLEANUP] –û—á–∏—Å—Ç–∫–∞ Redis –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        logger.info(f"[CLEANUP] - –£–¥–∞–ª–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {deleted_posts}")
        logger.info(f"[CLEANUP] - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {memory_info.get('used_memory_human', 'N/A')}")
        logger.info(f"[CLEANUP] - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–π: {memory_info.get('keys_count', 'N/A')}")
        
        return deleted_posts
    except Exception as e:
        logger.error(f"[CLEANUP] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ Redis: {e}")
        return 0


def is_word_match(text: str, words: list[Word]) -> bool:
    text = text.lower()
    for word in words:
        if word.title in text:
            return True

    return False


async def check_valid_photo(session: aiohttp.ClientSession, photo: str):
    try:
        async with session.get(photo, allow_redirects=True) as resp:
            return 200 <= resp.status < 400 and resp.content_type.startswith("image")
    except Exception:
        return False


def is_url(string: str):
    url_pattern = re.compile(
        r'^(https?://)?'
        r'([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}'
        r'(:[0-9]{1,5})?'
        r'(/\S*)?$'
    )

    return bool(url_pattern.match(string.strip()))


def is_at(string: str):
    return True if string.startswith("@") else False


def is_source_link(link_text: str) -> bool:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º.
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —É–∫–∞–∑–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
    """
    link_text = link_text.strip().lower()

    source_patterns = [
        "üîó –∏—Å—Ç–æ—á–Ω–∏–∫:",
        "–∏—Å—Ç–æ—á–Ω–∏–∫:",
        "source:",
        "–∏—Å—Ç–æ—á–Ω–∏–∫",
    ]

    for pattern in source_patterns:
        if link_text.startswith(pattern):
            return True

    for pattern in source_patterns:
        if pattern in link_text:
            return True

    return False


async def add_source_link(text: str, header: Tag):
    soup = BeautifulSoup(text, 'html.parser')
    channel_name = header.get_text().strip()
    link = header.get("href")

    entity = None
    if link:
        if '@' in link:
            entity = link.split("@")[-1]
        elif 't.me/' in link:
            after = link.split('t.me/')[-1]
            entity = after.split('/')[0]
        if entity:
            entity = entity.strip().strip('/')
            link = f"https://t.me/{entity}"

    source_link = soup.new_tag('a', href=link)
    source_link.string = f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {channel_name}"

    soup.append("\n\n")
    soup.append(source_link)

    return str(soup)


async def add_x_link(text: str, link: str, channel_rating: int = 0):
    soup = BeautifulSoup(text, 'html.parser')
    # link –ø—Ä–∏—Ö–æ–¥–∏—Ç –∫–∞–∫ "/username/status/123..." ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    normalized = link.lstrip('/')
    account_name = normalized.split('/')[0]
    link = f"https://x.com/{normalized}"
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–π—Ç–∏–Ω–≥ (–≤—Å–µ–≥–¥–∞)
    rating_text = f"‚≠ê{channel_rating}" if channel_rating > 0 else "‚ùå"
    rating_element = soup.new_string(f"–†–µ–π—Ç–∏–Ω–≥: {rating_text}\n")
    
    soup.append("\n\n")
    soup.append(rating_element)
    soup.append("\n")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
    try:
        if settings.get_source_x():
            source_link = soup.new_tag('a', href=link)
            source_link.string = f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {account_name}"
            soup.append(source_link)
    except Exception:
        # –ï—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        source_link = soup.new_tag('a', href=link)
        source_link.string = f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {account_name}"
        soup.append(source_link)

    return str(soup)


async def add_userbot_source_link(text: str, chat_title: str, chat_link: str, chat_id: int = None):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è —é–∑–µ—Ä–±–æ—Ç–∞.
    –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ —É–∂–µ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ, –Ω–∏—á–µ–≥–æ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ—Ç.
    """

    soup = BeautifulSoup(text, 'html.parser')

    text_lower = text.lower()
    source_patterns = ["üîó –∏—Å—Ç–æ—á–Ω–∏–∫:", "–∏—Å—Ç–æ—á–Ω–∏–∫:", "source:", "–∏—Å—Ç–æ—á–Ω–∏–∫"]

    for pattern in source_patterns:
        if pattern in text_lower:
            return text

    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥ (–≤—Å–µ–≥–¥–∞)
    rating = 0
    if chat_id:
        try:
            from app.database.repo.Chat import ChatRepo
            chat = await ChatRepo.get_by_telegram_id(chat_id)
            if chat:
                rating = chat.rating
        except Exception:
            pass

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–π—Ç–∏–Ω–≥ (–≤—Å–µ–≥–¥–∞)
    rating_text = f"‚≠ê{rating}" if rating > 0 else "‚ùå"
    rating_element = soup.new_string(f"–†–µ–π—Ç–∏–Ω–≥: {rating_text}\n")

    soup.append("\n\n")
    soup.append(rating_element)
    soup.append("\n")

    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
    try:
        if settings.get_source_tg():
            source_link = soup.new_tag('a', href=chat_link)
            source_link.string = f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {chat_title}"
            soup.append(source_link)
    except Exception:
        # –ï—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        source_link = soup.new_tag('a', href=chat_link)
        source_link.string = f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {chat_title}"
        soup.append(source_link)

    return str(soup)


def remove_links(text: str):
    """–£–¥–∞–ª—è–µ—Ç —Ç–µ–≥–∏ —Å—Å—ã–ª–æ–∫ –∏ –≥–æ–ª—ã–µ URL-—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
    soup = BeautifulSoup(text, "html.parser")

    # 1) –ü–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª—è–µ–º –≤—Å–µ —Ç–µ–≥–∏ <a>
    for a_tag in soup.find_all("a"):
        a_tag.decompose()

    text_content = str(soup)

    # 2) –£–¥–∞–ª—è–µ–º –≥–æ–ª—ã–µ URL-—ã (http/https, –∞ —Ç–∞–∫–∂–µ –¥–æ–º–µ–Ω—ã –±–µ–∑ —Å—Ö–µ–º—ã)
    url_regex = r"(https?://\S+|(?:www\.)?[a-zA-Z0-9-]+\.[a-zA-Z]{2,}(?:/\S*)?)"
    text_content = re.sub(url_regex, "", text_content)

    # 3) –ß–∏—Å—Ç–∏–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã/–ø–µ—Ä–µ–Ω–æ—Å—ã
    text_content = re.sub(r'\n\s*\n\s*\n', '\n\n', text_content)
    text_content = re.sub(r'[ \t]+', ' ', text_content)
    text_content = text_content.strip()

    return text_content


def remove_keywords(text: str, keyword):
    """
    –£–¥–∞–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    –†–∞–±–æ—Ç–∞–µ—Ç —Å HTML —Ä–∞–∑–º–µ—Ç–∫–æ–π –∏ —É–¥–∞–ª—è–µ—Ç —Å–ª–æ–≤–∞ —Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü —Å–ª–æ–≤.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫.
    """
    soup = BeautifulSoup(text, "html.parser")

    for element in soup.find_all(string=True):
        if not element.strip():
            continue

        new_text = element
        kw = keyword.title

        pattern = r'\b' + re.escape(kw) + r'\b[ \t,\.!?;:]*'

        new_text = re.sub(pattern, '', new_text, flags=re.IGNORECASE)

        new_text = re.sub(r'[ \t]+', ' ', new_text)
        new_text = re.sub(r'\n\s+', '\n', new_text)
        new_text = re.sub(r'\s+\n', '\n', new_text)

        element.replace_with(new_text)

    return str(soup)


async def preprocess_text(
    text: str,
    keyword: Word,
    allowed_tags=["a", "b", "i", "u", "s", "em", "code", "stroke", "br", "p"],
    allowed_attrs={"a": ["href"]},
    platform: str = "tg",  # "tg" –∏–ª–∏ "x"
) -> str:
    text = text.replace("<br/>", "\n").replace("<br>", "\n")
    text = text.replace("</p>", "\n").replace("<p>", "")
    text = html.unescape(text)

    text = bleach.clean(
        text,
        tags=allowed_tags,
        attributes=allowed_attrs,
        strip=True,
    )

    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ (—Ç–µ–≥–∏ –∏ –≥–æ–ª—ã–µ URL-—ã)
    text = remove_links(text)

    from app.database.repo.Word import WordRepo
    from app.enums import WordType
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ñ–∏–ª—å—Ç—Ä-—Å–ª–æ–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
    if platform == "tg":
        filter_words = await WordRepo.get_all(WordType.tg_filter_word)
    else:  # x
        filter_words = await WordRepo.get_all(WordType.x_filter_word)
    
    for filter_word in filter_words:
        text = remove_keywords(text, filter_word)

    text = re.sub(r'\n{3,}', '\n\n', text)
    text = text.strip()

    return text
