import re
import aiohttp
import bleach
import html
from bs4 import BeautifulSoup
from bs4.element import Tag
from app.database.models.Word import Word
from app.database.redis import redis_store
from app.settings import settings

POST_KEY = "post:{id}"


async def is_duplicate(id: str, original_text: str) -> bool:
    original_text = original_text.lower()
    from loguru import logger

    try:
        texts = await redis_store.values(POST_KEY.format(id="*"))
        logger.info(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –¥–ª—è ID: {id}, —Ç–µ–∫—Å—Ç–æ–≤ –≤ Redis: {len(texts) if texts else 0}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ texts –Ω–µ None –∏ –Ω–µ –ø—É—Å—Ç–æ–π
        if texts:
            for i, text in enumerate(texts):
                if text and original_text == text:
                    logger.info(f"–ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç! ID: {id}, —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Ç–µ–∫—Å—Ç–æ–º #{i}")
                    return True

        await redis_store.set_value_ex(POST_KEY.format(id=id), original_text, 60 * 60 * 24)
        logger.info(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –ø–æ—Å—Ç –≤ Redis: {id}")
        return False
    except Exception as e:
        # –ï—Å–ª–∏ Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ª–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")
        return False


async def get_fetched_post_ids():
    keys: list[str] = await redis_store.keys(POST_KEY.format(id="*"))
    return [key.split(":")[1] for key in keys]


def is_word_match(text: str, words: list[Word]) -> bool:
    text = text.lower()
    for word in words:
        if word.title in text:
            return True

    return False


async def check_valid_photo(session: aiohttp.ClientSession, photo: str):
    try:
        async with session.get(photo, allow_redirects=True) as resp:
            return 200 <= resp.status < 400 and resp.content_type.startswith("image")
    except Exception:
        return False


def is_url(string: str):
    url_pattern = re.compile(
        r'^(https?://)?'
        r'([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}'
        r'(:[0-9]{1,5})?'
        r'(/\S*)?$'
    )

    return bool(url_pattern.match(string.strip()))


def is_at(string: str):
    return True if string.startswith("@") else False


def is_source_link(link_text: str) -> bool:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º.
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —É–∫–∞–∑–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
    """
    link_text = link_text.strip().lower()

    source_patterns = [
        "üîó –∏—Å—Ç–æ—á–Ω–∏–∫:",
        "–∏—Å—Ç–æ—á–Ω–∏–∫:",
        "source:",
        "–∏—Å—Ç–æ—á–Ω–∏–∫",
    ]

    for pattern in source_patterns:
        if link_text.startswith(pattern):
            return True

    for pattern in source_patterns:
        if pattern in link_text:
            return True

    return False


async def add_source_link(text: str, header: Tag):
    soup = BeautifulSoup(text, 'html.parser')
    channel_name = header.get_text().strip()
    link = header.get("href")

    entity = None
    if link:
        if '@' in link:
            entity = link.split("@")[-1]
        elif 't.me/' in link:
            after = link.split('t.me/')[-1]
            entity = after.split('/')[0]
        if entity:
            entity = entity.strip().strip('/')
            link = f"https://t.me/{entity}"

    source_link = soup.new_tag('a', href=link)
    source_link.string = f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {channel_name}"

    soup.append("\n\n")
    soup.append(source_link)

    return str(soup)


async def add_x_link(text: str, link: str, channel_rating: int = 0):
    soup = BeautifulSoup(text, 'html.parser')
    # link –ø—Ä–∏—Ö–æ–¥–∏—Ç –∫–∞–∫ "/username/status/123..." ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    normalized = link.lstrip('/')
    account_name = normalized.split('/')[0]
    link = f"https://x.com/{normalized}"
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–π—Ç–∏–Ω–≥
    rating_text = f"‚≠ê{channel_rating}" if channel_rating > 0 else "‚ùå"
    rating_element = soup.new_string(f"–†–µ–π—Ç–∏–Ω–≥: {rating_text}\n")
    
    source_link = soup.new_tag('a', href=link)
    source_link.string = f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {account_name}"
    
    soup.append("\n\n")
    soup.append(rating_element)
    soup.append("\n")
    soup.append(source_link)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É: –Ω—É–∂–Ω–æ –ª–∏ –¥–æ–±–∞–≤–ª—è—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è X
    try:
        
        if not settings.get_source_x():
            return text
    except Exception:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫—É ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
        pass

    return str(soup)


async def add_userbot_source_link(text: str, chat_title: str, chat_link: str, chat_id: int = None):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è —é–∑–µ—Ä–±–æ—Ç–∞.
    –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ —É–∂–µ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ, –Ω–∏—á–µ–≥–æ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ—Ç.
    """

    soup = BeautifulSoup(text, 'html.parser')

    text_lower = text.lower()
    source_patterns = ["üîó –∏—Å—Ç–æ—á–Ω–∏–∫:", "–∏—Å—Ç–æ—á–Ω–∏–∫:", "source:", "–∏—Å—Ç–æ—á–Ω–∏–∫"]

    for pattern in source_patterns:
        if pattern in text_lower:
            return text

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É: –Ω—É–∂–Ω–æ –ª–∏ –¥–æ–±–∞–≤–ª—è—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è Telegram
    try:
        from app.settings import settings
        if not settings.get_source_tg():
            return text
    except Exception:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫—É ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
        pass

    rating = 0
    if chat_id:
        try:
            from app.database.repo.Chat import ChatRepo
            chat = await ChatRepo.get_by_telegram_id(chat_id)
            if chat:
                rating = chat.rating
        except Exception:
            pass

    source_link = soup.new_tag('a', href=chat_link)

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–π—Ç–∏–Ω–≥
    rating_text = f"‚≠ê{rating}" if rating > 0 else "‚ùå"
    rating_element = soup.new_string(f"–†–µ–π—Ç–∏–Ω–≥: {rating_text}\n")

    # rating_text = f" (‚≠ê{rating})"
    source_link.string = f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {chat_title}"

    soup.append("\n\n")
    soup.append(rating_element)
    soup.append("\n")
    soup.append(source_link)

    return str(soup)


def remove_links(text: str):
    """–£–¥–∞–ª—è–µ—Ç —Ç–µ–≥–∏ —Å—Å—ã–ª–æ–∫ –∏ –≥–æ–ª—ã–µ URL-—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
    soup = BeautifulSoup(text, "html.parser")

    # 1) –ü–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª—è–µ–º –≤—Å–µ —Ç–µ–≥–∏ <a>
    for a_tag in soup.find_all("a"):
        a_tag.decompose()

    text_content = str(soup)

    # 2) –£–¥–∞–ª—è–µ–º –≥–æ–ª—ã–µ URL-—ã (http/https, –∞ —Ç–∞–∫–∂–µ –¥–æ–º–µ–Ω—ã –±–µ–∑ —Å—Ö–µ–º—ã)
    url_regex = r"(https?://\S+|(?:www\.)?[a-zA-Z0-9-]+\.[a-zA-Z]{2,}(?:/\S*)?)"
    text_content = re.sub(url_regex, "", text_content)

    # 3) –ß–∏—Å—Ç–∏–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã/–ø–µ—Ä–µ–Ω–æ—Å—ã
    text_content = re.sub(r'\n\s*\n\s*\n', '\n\n', text_content)
    text_content = re.sub(r'[ \t]+', ' ', text_content)
    text_content = text_content.strip()

    return text_content


def remove_keywords(text: str, keyword):
    """
    –£–¥–∞–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    –†–∞–±–æ—Ç–∞–µ—Ç —Å HTML —Ä–∞–∑–º–µ—Ç–∫–æ–π –∏ —É–¥–∞–ª—è–µ—Ç —Å–ª–æ–≤–∞ —Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü —Å–ª–æ–≤.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫.
    """
    soup = BeautifulSoup(text, "html.parser")

    for element in soup.find_all(string=True):
        if not element.strip():
            continue

        new_text = element
        kw = keyword.title

        pattern = r'\b' + re.escape(kw) + r'\b[ \t,\.!?;:]*'

        new_text = re.sub(pattern, '', new_text, flags=re.IGNORECASE)

        new_text = re.sub(r'[ \t]+', ' ', new_text)
        new_text = re.sub(r'\n\s+', '\n', new_text)
        new_text = re.sub(r'\s+\n', '\n', new_text)

        element.replace_with(new_text)

    return str(soup)


async def preprocess_text(
    text: str,
    keyword: Word,
    allowed_tags=["a", "b", "i", "u", "s", "em", "code", "stroke", "br", "p"],
    allowed_attrs={"a": ["href"]},
    platform: str = "tg",  # "tg" –∏–ª–∏ "x"
) -> str:
    text = text.replace("<br/>", "\n").replace("<br>", "\n")
    text = text.replace("</p>", "\n").replace("<p>", "")
    text = html.unescape(text)

    text = bleach.clean(
        text,
        tags=allowed_tags,
        attributes=allowed_attrs,
        strip=True,
    )

    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ (—Ç–µ–≥–∏ –∏ –≥–æ–ª—ã–µ URL-—ã)
    text = remove_links(text)

    from app.database.repo.Word import WordRepo
    from app.enums import WordType
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ñ–∏–ª—å—Ç—Ä-—Å–ª–æ–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
    if platform == "tg":
        filter_words = await WordRepo.get_all(WordType.tg_filter_word)
    else:  # x
        filter_words = await WordRepo.get_all(WordType.x_filter_word)
    
    for filter_word in filter_words:
        text = remove_keywords(text, filter_word)

    text = re.sub(r'\n{3,}', '\n\n', text)
    text = text.strip()

    return text
